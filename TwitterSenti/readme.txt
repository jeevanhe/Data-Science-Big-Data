Use Twitter Search API to download tweets about anything that you care about.
For example, you could search for "UTD" and get a list of tweets.
Additionally, you have to search for the same topic for 6 different timelines
to get 6 different files. For example, you can search for UTD on 6 different
days to get 6 input files for MapReduce program to process. The MapReduce
program will then look for Hashtags i.e. those words starting with "#" to find
which Hashtags are trending for a topic.

Here are the requirements: 1. You have to use the API i.e. sign up for a
developer account and use a programming language (preferably Java, but not
required) to search for tweets. You might find some useful information here:
https://dev.twitter.com/rest/public/search

2. You have to search for the same topic on 6 different timelines. It is up to
you to choose time intervals e.g. one day, one week, etc. But you need to get
6 medium sized input files containing tweets. Remember to search for a topic
and not a hashtag.

3. After getting the 6 input files, you can run the MapReduce code on them.
The only change that you need to make is that instead of emitting a key value
pair for each word, you have to look for Hashtags i.e. words starting with "#"
and emit key value pair for them. You only need to make this change in the map
method of the mapper class.

4. Get the output of reducer as usual.

Twitter Search API is used to download tweets about “newyear”. The tweets are
downloaded for each day from 12/31/2015 to 02/03/2016. The tweet count is set
to 100 for each file so as to get a medium sized file. Each tweet file is of
approximately 10kb size.

>Files generated by running TwitterTweetDownload.java

tweetfile1.txt tweetfile2.txt tweetfile3.txt tweetfile4.txt tweetfile5.txt
tweetfile6.txt


>These files were copied to UTD cluster using WinSCP
>These files were copied to hdfs using the –copyFromLocal command.
	hdfs dfs -copyFromLocal tweetfile1.txt /user/jhe140030/twitter
>create a new directory
	hdfs dfs -mkdir twitter
>Execute jar file.
	hadoop jar TwitterSenti-0.0.1-SNAPSHOT.jar TwitterSenti.TwitterSenti.TweetWordCount /user/jhe140030/twitter /user/jhe140030/twitter_out
>Fetch output file (Map reduced output file)
	hdfs dfs –get /user/jhe140030/twitter_out/part-r-00000
>Displaying results	
	vim part-r-00000
	cat part-r-00000 > tweet_result.txt
